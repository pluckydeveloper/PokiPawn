#!/usr/bin/env python3
"""
Comprehensive Pokemon Sites Scraper
====================================
Scrapes 10 major Pokemon websites with complete data extraction including:
- All Pokemon data (stats, names, types, IDs, descriptions)
- Graphics (sprites, artwork, images, icons)
- Animations (GIFs, CSS animations, JavaScript interactions)
- UI/UX elements and site architecture
- Interactive elements and card animations

Target Sites:
1. pokemondb.net/pokedex/all
2. cardmarket.com/en/Pokemon  
3. pokemondb.net/pokedex
4. bulbapedia.bulbagarden.net/wiki/List_of_PokÃ©mon_by_National_PokÃ©dex_number
5. serebii.net/pokemon/nationalpokedex.shtml
6. ph.portal-pokemon.com/play/pokedex
7. pkmn.gg/pokedex
8. pkmn.gg/series
9. artofpkm.com/
10. tcg.pokemon.com/en-us/all-galleries/
"""

import os
import re
import json
import time
import requests
import threading
from pathlib import Path
from datetime import datetime
from urllib.parse import urljoin, urlparse, quote
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Optional, Set, Any

# Selenium imports for dynamic content
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
from webdriver_manager.chrome import ChromeDriverManager

# BeautifulSoup for HTML parsing
from bs4 import BeautifulSoup

class ComprehensivePokemonScraper:
    def __init__(self, output_dir="pokemon_comprehensive_scrape"):
        self.start_time = datetime.now()
        self.output_dir = Path(output_dir)
        self.session = requests.Session()
        self.downloaded_files = set()
        self.scraping_log = []
        self.lock = threading.Lock()
        
        # Global statistics
        self.stats = {
            'total_files_downloaded': 0,
            'total_size_mb': 0,
            'sites_completed': 0,
            'failed_downloads': 0,
            'animations_captured': 0,
            'interactions_recorded': 0
        }
        
        # Target sites configuration
        self.target_sites = {
            'pokemondb_all': {
                'url': 'https://pokemondb.net/pokedex/all',
                'name': 'Pokemon Database - Complete List',
                'type': 'static_with_dynamic',
                'priority': 'high',
                'description': 'Complete Pokemon database with stats and sprites'
            },
            'cardmarket': {
                'url': 'https://www.cardmarket.com/en/Pokemon',
                'name': 'Card Market Pokemon',
                'type': 'dynamic',
                'priority': 'high',
                'description': 'Pokemon card trading marketplace'
            },
            'pokemondb_pokedex': {
                'url': 'https://pokemondb.net/pokedex',
                'name': 'Pokemon Database - Individual Pages',
                'type': 'static_with_dynamic',
                'priority': 'high',
                'description': 'Individual Pokemon pages with detailed data'
            },
            'bulbapedia': {
                'url': 'https://bulbapedia.bulbagarden.net/wiki/List_of_PokÃ©mon_by_National_PokÃ©dex_number',
                'name': 'Bulbapedia Pokemon List',
                'type': 'static',
                'priority': 'medium',
                'description': 'Comprehensive Pokemon wiki data'
            },
            'serebii': {
                'url': 'https://www.serebii.net/pokemon/nationalpokedex.shtml',
                'name': 'Serebii National Pokedex',
                'type': 'static',
                'priority': 'medium',
                'description': 'National Pokedex with game data'
            },
            'portal_pokemon': {
                'url': 'https://ph.portal-pokemon.com/play/pokedex',
                'name': 'Portal Pokemon Pokedex',
                'type': 'dynamic',
                'priority': 'medium',
                'description': 'Interactive Pokedex interface'
            },
            'pkmn_pokedex': {
                'url': 'https://www.pkmn.gg/pokedex',
                'name': 'PKMN.GG Pokedex',
                'type': 'dynamic',
                'priority': 'high',
                'description': 'Modern Pokemon database interface'
            },
            'pkmn_series': {
                'url': 'https://www.pkmn.gg/series',
                'name': 'PKMN.GG Series',
                'type': 'dynamic',
                'priority': 'medium',
                'description': 'Pokemon series and game data'
            },
            'artofpkm': {
                'url': 'https://www.artofpkm.com/',
                'name': 'Art of Pokemon',
                'type': 'static_with_dynamic',
                'priority': 'high',
                'description': 'Pokemon artwork and illustrations'
            },
            'tcg_galleries': {
                'url': 'https://tcg.pokemon.com/en-us/all-galleries/',
                'name': 'Pokemon TCG Galleries',
                'type': 'dynamic_animations',
                'priority': 'critical',
                'description': 'Pokemon trading card animations and galleries'
            }
        }
        
        # Setup session headers
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        # Create directory structure
        self.create_master_directory_structure()
        self.initialize_reference_documentation()
        
    def create_master_directory_structure(self):
        """Create organized directory structure for each site"""
        print("ğŸ—ï¸  Creating comprehensive directory structure...")
        
        for site_key, site_info in self.target_sites.items():
            site_dir = self.output_dir / site_key
            
            # Create main directories
            directories = [
                'data',           # JSON/CSV data files
                'images',         # Static images and sprites
                'animations',     # GIFs and animation files
                'videos',         # Recorded interactions and animations
                'css',           # Stylesheets and visual assets
                'javascript',    # JavaScript files and interactions
                'pages',         # HTML pages
                'screenshots',   # UI screenshots
                'interactions',  # Interactive element recordings
                'reference'      # Site-specific documentation
            ]
            
            for directory in directories:
                (site_dir / directory).mkdir(parents=True, exist_ok=True)
        
        # Create master directories
        master_dirs = ['master_reference', 'consolidated_data', 'analysis']
        for directory in master_dirs:
            (self.output_dir / directory).mkdir(parents=True, exist_ok=True)
            
        print(f"âœ… Directory structure created in: {self.output_dir}")
    
    def initialize_reference_documentation(self):
        """Initialize master reference documentation"""
        self.master_reference = {
            'scraping_session': {
                'start_time': self.start_time.isoformat(),
                'scraper_version': '1.0.0',
                'total_sites': len(self.target_sites),
                'operation_type': 'comprehensive_extraction'
            },
            'target_sites': self.target_sites,
            'extraction_progress': {},
            'data_inventory': {},
            'animations_captured': {},
            'interactions_recorded': {},
            'technical_details': {}
        }
        
    def setup_selenium_driver(self, headless=False):
        """Setup Selenium WebDriver with optimized settings"""
        chrome_options = Options()
        
        if headless:
            chrome_options.add_argument('--headless')
            
        # Performance optimizations
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # Enable downloads
        prefs = {
            "profile.default_content_settings.popups": 0,
            "download.default_directory": str(self.output_dir.absolute()),
            "download.prompt_for_download": False,
        }
        chrome_options.add_experimental_option("prefs", prefs)
        
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        # Execute script to avoid detection
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        return driver
    
    def log_progress(self, site_key: str, message: str, level: str = "INFO"):
        """Log scraping progress with timestamps"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = {
            'timestamp': timestamp,
            'site': site_key,
            'message': message,
            'level': level
        }
        
        with self.lock:
            self.scraping_log.append(log_entry)
            
        # Console output with colors
        colors = {
            'INFO': '\033[94m',     # Blue
            'SUCCESS': '\033[92m',  # Green
            'WARNING': '\033[93m',  # Yellow
            'ERROR': '\033[91m',    # Red
            'CRITICAL': '\033[95m'  # Magenta
        }
        
        reset_color = '\033[0m'
        color = colors.get(level, '')
        
        print(f"{color}[{timestamp}] {site_key.upper()}: {message}{reset_color}")
        
    def download_file(self, url: str, local_path: Path, site_key: str, file_type: str = "unknown") -> bool:
        """Download file with progress tracking"""
        try:
            if url in self.downloaded_files:
                return True
                
            response = self.session.get(url, timeout=30, stream=True)
            response.raise_for_status()
            
            # Create directory if needed
            local_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Download with progress
            total_size = int(response.headers.get('content-length', 0))
            
            with open(local_path, 'wb') as f:
                downloaded = 0
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
            
            with self.lock:
                self.downloaded_files.add(url)
                self.stats['total_files_downloaded'] += 1
                file_size_mb = local_path.stat().st_size / (1024 * 1024)
                self.stats['total_size_mb'] += file_size_mb
                
            self.log_progress(site_key, f"Downloaded {file_type}: {local_path.name} ({file_size_mb:.2f}MB)")
            return True
            
        except Exception as e:
            self.log_progress(site_key, f"Failed to download {url}: {e}", "ERROR")
            with self.lock:
                self.stats['failed_downloads'] += 1
            return False
    
    def start_comprehensive_scraping(self):
        """Start the comprehensive scraping operation"""
        print("\n" + "="*80)
        print("ğŸ® COMPREHENSIVE POKEMON SITES SCRAPING OPERATION")
        print("="*80)
        print(f"ğŸ“… Started: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ¯ Target Sites: {len(self.target_sites)}")
        print(f"ğŸ’¾ Output Directory: {self.output_dir.absolute()}")
        print("="*80)
        
        # Process sites by priority
        priority_order = ['critical', 'high', 'medium', 'low']
        
        for priority in priority_order:
            sites_in_priority = [
                (key, info) for key, info in self.target_sites.items() 
                if info['priority'] == priority
            ]
            
            if sites_in_priority:
                print(f"\nğŸ”¥ Processing {priority.upper()} priority sites...")
                
                for site_key, site_info in sites_in_priority:
                    self.scrape_single_site(site_key, site_info)
                    self.stats['sites_completed'] += 1
                    
                    # Update master reference after each site
                    self.update_master_reference(site_key)
        
        # Generate final comprehensive report
        self.generate_final_report()
        
    def scrape_single_site(self, site_key: str, site_info: Dict):
        """Scrape a single site with appropriate strategy"""
        print(f"\nğŸŒ Starting {site_info['name']} ({site_key})")
        print(f"ğŸ”— URL: {site_info['url']}")
        print(f"ğŸ“‹ Type: {site_info['type']}")
        
        site_start_time = datetime.now()
        
        try:
            # Choose scraping strategy based on site type
            if site_info['type'] == 'static':
                success = self.scrape_static_site(site_key, site_info)
            elif site_info['type'] == 'dynamic':
                success = self.scrape_dynamic_site(site_key, site_info)
            elif site_info['type'] == 'static_with_dynamic':
                success = self.scrape_hybrid_site(site_key, site_info)
            elif site_info['type'] == 'dynamic_animations':
                success = self.scrape_animation_site(site_key, site_info)
            else:
                success = self.scrape_hybrid_site(site_key, site_info)
                
            elapsed = datetime.now() - site_start_time
            
            if success:
                self.log_progress(site_key, f"Site completed successfully in {elapsed}", "SUCCESS")
            else:
                self.log_progress(site_key, f"Site completed with issues in {elapsed}", "WARNING")
                
        except Exception as e:
            elapsed = datetime.now() - site_start_time
            self.log_progress(site_key, f"Site failed after {elapsed}: {e}", "ERROR")
    
    def scrape_static_site(self, site_key: str, site_info: Dict) -> bool:
        """Scrape static sites using requests and BeautifulSoup"""
        self.log_progress(site_key, "Starting static site scraping...")
        
        try:
            # Get main page
            response = self.session.get(site_info['url'], timeout=30)
            response.raise_for_status()
            
            # Save main page
            main_page_path = self.output_dir / site_key / 'pages' / 'index.html'
            with open(main_page_path, 'w', encoding='utf-8') as f:
                f.write(response.text)
            
            # Parse with BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract all media and assets
            self.extract_static_assets(soup, site_key, site_info['url'])
            
            # Extract Pokemon data if applicable
            pokemon_data = self.extract_pokemon_data_static(soup, site_key)
            if pokemon_data:
                data_file = self.output_dir / site_key / 'data' / 'pokemon_data.json'
                with open(data_file, 'w', encoding='utf-8') as f:
                    json.dump(pokemon_data, f, indent=2, ensure_ascii=False)
            
            return True
            
        except Exception as e:
            self.log_progress(site_key, f"Static scraping failed: {e}", "ERROR")
            return False
    
    def scrape_dynamic_site(self, site_key: str, site_info: Dict) -> bool:
        """Scrape dynamic sites using Selenium"""
        self.log_progress(site_key, "Starting dynamic site scraping...")
        
        driver = None
        try:
            driver = self.setup_selenium_driver()
            driver.get(site_info['url'])
            
            # Wait for page to load
            WebDriverWait(driver, 20).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # Let dynamic content load
            time.sleep(5)
            
            # Scroll to trigger lazy loading
            self.trigger_lazy_loading(driver, site_key)
            
            # Extract dynamic content
            self.extract_dynamic_content(driver, site_key, site_info['url'])
            
            # Take screenshots of different states
            self.capture_ui_states(driver, site_key)
            
            return True
            
        except Exception as e:
            self.log_progress(site_key, f"Dynamic scraping failed: {e}", "ERROR")
            return False
        finally:
            if driver:
                driver.quit()
    
    def scrape_hybrid_site(self, site_key: str, site_info: Dict) -> bool:
        """Scrape sites with both static and dynamic content"""
        self.log_progress(site_key, "Starting hybrid site scraping...")
        
        # First do static scraping
        static_success = self.scrape_static_site(site_key, site_info)
        
        # Then do dynamic scraping for interactive elements
        dynamic_success = self.scrape_dynamic_site(site_key, site_info)
        
        return static_success and dynamic_success
    
    def scrape_animation_site(self, site_key: str, site_info: Dict) -> bool:
        """Scrape sites with special focus on animations and interactions"""
        self.log_progress(site_key, "Starting animation-focused scraping...")
        
        driver = None
        try:
            driver = self.setup_selenium_driver()
            driver.get(site_info['url'])
            
            # Wait for page to load
            WebDriverWait(driver, 30).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # Special handling for TCG card animations
            if 'tcg.pokemon.com' in site_info['url']:
                return self.scrape_tcg_animations(driver, site_key)
            
            # General animation capture
            return self.capture_site_animations(driver, site_key)
            
        except Exception as e:
            self.log_progress(site_key, f"Animation scraping failed: {e}", "ERROR")
            return False
        finally:
            if driver:
                driver.quit()
    
    def extract_static_assets(self, soup: BeautifulSoup, site_key: str, base_url: str):
        """Extract all static assets from parsed HTML"""
        self.log_progress(site_key, "Extracting static assets...")
        
        # Extract images
        images = soup.find_all('img')
        self.log_progress(site_key, f"Found {len(images)} images to download")
        
        for img in images:
            src = img.get('src')
            if src:
                img_url = urljoin(base_url, src)
                filename = self.get_safe_filename(img_url)
                local_path = self.output_dir / site_key / 'images' / filename
                self.download_file(img_url, local_path, site_key, "image")
        
        # Extract CSS files
        css_links = soup.find_all('link', {'rel': 'stylesheet'})
        self.log_progress(site_key, f"Found {len(css_links)} CSS files to download")
        
        for css_link in css_links:
            href = css_link.get('href')
            if href:
                css_url = urljoin(base_url, href)
                filename = self.get_safe_filename(css_url)
                local_path = self.output_dir / site_key / 'css' / filename
                self.download_file(css_url, local_path, site_key, "CSS")
        
        # Extract JavaScript files
        scripts = soup.find_all('script', src=True)
        self.log_progress(site_key, f"Found {len(scripts)} JavaScript files to download")
        
        for script in scripts:
            src = script.get('src')
            if src:
                js_url = urljoin(base_url, src)
                filename = self.get_safe_filename(js_url)
                local_path = self.output_dir / site_key / 'javascript' / filename
                self.download_file(js_url, local_path, site_key, "JavaScript")
    
    def get_safe_filename(self, url: str) -> str:
        """Generate safe filename from URL"""
        parsed = urlparse(url)
        filename = os.path.basename(parsed.path)
        
        if not filename or '.' not in filename:
            filename = f"file_{abs(hash(url))}.html"
        
        # Clean filename
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        return filename
    
    def update_master_reference(self, site_key: str):
        """Update master reference documentation after completing a site"""
        # Save progress to master reference file
        reference_file = self.output_dir / 'master_reference' / 'comprehensive_scraping_reference.json'
        
        self.master_reference['extraction_progress'][site_key] = {
            'completed': True,
            'completion_time': datetime.now().isoformat(),
            'files_downloaded': self.stats['total_files_downloaded'],
            'total_size_mb': round(self.stats['total_size_mb'], 2)
        }
        
        with open(reference_file, 'w', encoding='utf-8') as f:
            json.dump(self.master_reference, f, indent=2, ensure_ascii=False)
    
    def extract_pokemon_data_static(self, soup: BeautifulSoup, site_key: str) -> List[Dict]:
        """Extract Pokemon data from static HTML"""
        # This will be implemented with site-specific logic
        return []
    
    def trigger_lazy_loading(self, driver, site_key: str):
        """Trigger lazy loading by scrolling"""
        self.log_progress(site_key, "Triggering lazy loading...")
        
        # Scroll down gradually
        for i in range(5):
            driver.execute_script(f"window.scrollTo(0, {(i+1) * 1000});")
            time.sleep(2)
        
        # Scroll to bottom
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(3)
    
    def extract_dynamic_content(self, driver, site_key: str, base_url: str):
        """Extract content from dynamic pages"""
        # This will be implemented with site-specific logic
        pass
    
    def capture_ui_states(self, driver, site_key: str):
        """Capture screenshots of different UI states"""
        screenshots_dir = self.output_dir / site_key / 'screenshots'
        
        # Main page screenshot
        screenshot_path = screenshots_dir / 'main_page.png'
        driver.save_screenshot(str(screenshot_path))
        self.log_progress(site_key, "Captured main page screenshot")
    
    def scrape_tcg_animations(self, driver, site_key: str) -> bool:
        """Special handling for TCG card animations"""
        self.log_progress(site_key, "Capturing TCG card animations...")
        # This will be implemented with specific TCG logic
        return True
    
    def capture_site_animations(self, driver, site_key: str) -> bool:
        """Capture general site animations"""
        self.log_progress(site_key, "Capturing site animations...")
        # This will be implemented with animation capture logic
        return True
    
    def generate_final_report(self):
        """Generate comprehensive final report"""
        end_time = datetime.now()
        total_duration = end_time - self.start_time
        
        print("\n" + "="*80)
        print("ğŸ COMPREHENSIVE SCRAPING OPERATION COMPLETED")
        print("="*80)
        print(f"â±ï¸  Total Duration: {total_duration}")
        print(f"ğŸŒ Sites Completed: {self.stats['sites_completed']}/{len(self.target_sites)}")
        print(f"ğŸ“ Files Downloaded: {self.stats['total_files_downloaded']:,}")
        print(f"ğŸ’¾ Total Size: {self.stats['total_size_mb']:.2f}MB")
        print(f"âŒ Failed Downloads: {self.stats['failed_downloads']}")
        print(f"ğŸ¬ Animations Captured: {self.stats['animations_captured']}")
        print(f"ğŸ–±ï¸  Interactions Recorded: {self.stats['interactions_recorded']}")
        print("="*80)
        
        # Save final reference document
        final_reference = {
            **self.master_reference,
            'operation_summary': {
                'end_time': end_time.isoformat(),
                'total_duration_seconds': total_duration.total_seconds(),
                'final_stats': self.stats,
                'scraping_log': self.scraping_log
            }
        }
        
        final_reference_file = self.output_dir / 'master_reference' / 'FINAL_COMPREHENSIVE_REFERENCE.json'
        with open(final_reference_file, 'w', encoding='utf-8') as f:
            json.dump(final_reference, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ Final reference saved: {final_reference_file}")

def main():
    """Main execution function"""
    scraper = ComprehensivePokemonScraper()
    scraper.start_comprehensive_scraping()

if __name__ == "__main__":
    main() 